{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3a04ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import seaborn as sns # visualization library\n",
    "import matplotlib.pyplot as plt # visualization library\n",
    "import plotly.plotly as py # visualization library\n",
    "from plotly.offline import init_notebook_mode, iplot # plotly offline mode\n",
    "init_notebook_mode(connected=True) \n",
    "import plotly.graph_objs as go # plotly graphical object\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "# import warnings library\n",
    "import warnings        \n",
    "# ignore filters\n",
    "warnings.filterwarnings(\"ignore\") # if there is a warning after some codes, this will avoid us to see them.\n",
    "plt.style.use('ggplot') # style of plots. ggplot is one of the most used style, I also like it.\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0b29b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the Data\n",
    "# bombing data\n",
    "aerial = pd.read_csv(\"../input/world-war-ii/operations.csv\")\n",
    "# first weather data that includes locations like country, latitude and longitude.\n",
    "weather_station_location = pd.read_csv(\"../input/weatherww2/Weather Station Locations.csv\")\n",
    "# Second weather data that includes measured min, max and mean temperatures\n",
    "weather = pd.read_csv(\"../input/weatherww2/Summary of Weather.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee58ca5",
   "metadata": {},
   "source": [
    "\n",
    "# Data Cleaning\n",
    "Aerial Bombing data includes a lot of NaN value. Instead of usign them, I drop some NaN values. It does not only remove the uncertainty but it also easa visualization process.\n",
    "\n",
    "Drop countries that are NaN\n",
    "\n",
    "Drop if target longitude is NaN\n",
    "\n",
    "Drop if takeoff longitude is NaN\n",
    "\n",
    "Drop unused features\n",
    "\n",
    "Weather Condition data does not need any cleaning. According to exploratory data analysis and visualization, we will choose certain location to examine deeper. However, lets put our data variables what we use only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b19ff83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop countries that are NaN\n",
    "aerial = aerial[pd.isna(aerial.Country)==False]\n",
    "# drop if target longitude is NaN\n",
    "aerial = aerial[pd.isna(aerial['Target Longitude'])==False]\n",
    "# Drop if takeoff longitude is NaN\n",
    "aerial = aerial[pd.isna(aerial['Takeoff Longitude'])==False]\n",
    "# drop unused features\n",
    "drop_list = ['Mission ID','Unit ID','Target ID','Altitude (Hundreds of Feet)','Airborne Aircraft',\n",
    "             'Attacking Aircraft', 'Bombing Aircraft', 'Aircraft Returned',\n",
    "             'Aircraft Failed', 'Aircraft Damaged', 'Aircraft Lost',\n",
    "             'High Explosives', 'High Explosives Type','Mission Type',\n",
    "             'High Explosives Weight (Pounds)', 'High Explosives Weight (Tons)',\n",
    "             'Incendiary Devices', 'Incendiary Devices Type',\n",
    "             'Incendiary Devices Weight (Pounds)',\n",
    "             'Incendiary Devices Weight (Tons)', 'Fragmentation Devices',\n",
    "             'Fragmentation Devices Type', 'Fragmentation Devices Weight (Pounds)',\n",
    "             'Fragmentation Devices Weight (Tons)', 'Total Weight (Pounds)',\n",
    "             'Total Weight (Tons)', 'Time Over Target', 'Bomb Damage Assessment','Source ID']\n",
    "aerial.drop(drop_list, axis=1,inplace = True)\n",
    "aerial = aerial[ aerial.iloc[:,8]!=\"4248\"] # drop this takeoff latitude \n",
    "aerial = aerial[ aerial.iloc[:,9]!=1355]   # drop this takeoff longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f55bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "aerial.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2874bde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what we will use only\n",
    "weather_station_location = weather_station_location.loc[:,[\"WBAN\",\"NAME\",\"STATE/COUNTRY ID\",\"Latitude\",\"Longitude\"] ]\n",
    "weather_station_location.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e52879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what we will use only\n",
    "weather = weather.loc[:,[\"STA\",\"Date\",\"MeanTemp\"] ]\n",
    "weather.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11c580c",
   "metadata": {},
   "source": [
    "\n",
    "# Data Visualization\n",
    "Lets start with basics of visualization that is understanding data.\n",
    "\n",
    "How many country which attacks\n",
    "\n",
    "Top target countries\n",
    "\n",
    "Top 10 aircraft series\n",
    "\n",
    "Takeoff base locations (Attacjk countries)\n",
    "\n",
    "Target locations (If you do not understand methods of pyplot look at my pyplot tutorial: https://www.kaggle.com/kanncaa1/plotly-tutorial-for-beginners)\n",
    "\n",
    "Bombing paths\n",
    "\n",
    "Theater of Operations\n",
    "\n",
    "Weather station locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a50bdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# country\n",
    "print(aerial['Country'].value_counts())\n",
    "plt.figure(figsize=(22,10))\n",
    "sns.countplot(aerial['Country'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b426008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top target countries\n",
    "print(aerial['Target Country'].value_counts()[:10])\n",
    "plt.figure(figsize=(22,10))\n",
    "sns.countplot(aerial['Target Country'])\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d17733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aircraft Series\n",
    "data = aerial['Aircraft Series'].value_counts()\n",
    "print(data[:10])\n",
    "data = [go.Bar(\n",
    "            x=data[:10].index,\n",
    "            y=data[:10].values,\n",
    "            hoverinfo = 'text',\n",
    "            marker = dict(color = 'rgba(177, 14, 22, 0.5)',\n",
    "                             line=dict(color='rgb(0,0,0)',width=1.5)),\n",
    "    )]\n",
    "\n",
    "layout = dict(\n",
    "    title = 'Aircraft Series',\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2bd812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ATTACK\n",
    "aerial[\"color\"] = \"\"\n",
    "aerial.color[aerial.Country == \"USA\"] = \"rgb(0,116,217)\"\n",
    "aerial.color[aerial.Country == \"GREAT BRITAIN\"] = \"rgb(255,65,54)\"\n",
    "aerial.color[aerial.Country == \"NEW ZEALAND\"] = \"rgb(133,20,75)\"\n",
    "aerial.color[aerial.Country == \"SOUTH AFRICA\"] = \"rgb(255,133,27)\"\n",
    "\n",
    "data = [dict(\n",
    "    type='scattergeo',\n",
    "    lon = aerial['Takeoff Longitude'],\n",
    "    lat = aerial['Takeoff Latitude'],\n",
    "    hoverinfo = 'text',\n",
    "    text = \"Country: \" + aerial.Country + \" Takeoff Location: \"+aerial[\"Takeoff Location\"]+\" Takeoff Base: \" + aerial['Takeoff Base'],\n",
    "    mode = 'markers',\n",
    "    marker=dict(\n",
    "        sizemode = 'area',\n",
    "        sizeref = 1,\n",
    "        size= 10 ,\n",
    "        line = dict(width=1,color = \"white\"),\n",
    "        color = aerial[\"color\"],\n",
    "        opacity = 0.7),\n",
    ")]\n",
    "layout = dict(\n",
    "    title = 'Countries Take Off Bases ',\n",
    "    hovermode='closest',\n",
    "    geo = dict(showframe=False, showland=True, showcoastlines=True, showcountries=True,\n",
    "               countrywidth=1, projection=dict(type='Mercator'),\n",
    "              landcolor = 'rgb(217, 217, 217)',\n",
    "              subunitwidth=1,\n",
    "              showlakes = True,\n",
    "              lakecolor = 'rgb(255, 255, 255)',\n",
    "              countrycolor=\"rgb(5, 5, 5)\")\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2e88ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bombing paths\n",
    "# trace1\n",
    "airports = [ dict(\n",
    "        type = 'scattergeo',\n",
    "        lon = aerial['Takeoff Longitude'],\n",
    "        lat = aerial['Takeoff Latitude'],\n",
    "        hoverinfo = 'text',\n",
    "        text = \"Country: \" + aerial.Country + \" Takeoff Location: \"+aerial[\"Takeoff Location\"]+\" Takeoff Base: \" + aerial['Takeoff Base'],\n",
    "        mode = 'markers',\n",
    "        marker = dict( \n",
    "            size=5, \n",
    "            color = aerial[\"color\"],\n",
    "            line = dict(\n",
    "                width=1,\n",
    "                color = \"white\"\n",
    "            )\n",
    "        ))]\n",
    "# trace2\n",
    "targets = [ dict(\n",
    "        type = 'scattergeo',\n",
    "        lon = aerial['Target Longitude'],\n",
    "        lat = aerial['Target Latitude'],\n",
    "        hoverinfo = 'text',\n",
    "        text = \"Target Country: \"+aerial[\"Target Country\"]+\" Target City: \"+aerial[\"Target City\"],\n",
    "        mode = 'markers',\n",
    "        marker = dict( \n",
    "            size=1, \n",
    "            color = \"red\",\n",
    "            line = dict(\n",
    "                width=0.5,\n",
    "                color = \"red\"\n",
    "            )\n",
    "        ))]\n",
    "        \n",
    "# trace3\n",
    "flight_paths = []\n",
    "for i in range( len( aerial['Target Longitude'] ) ):\n",
    "    flight_paths.append(\n",
    "        dict(\n",
    "            type = 'scattergeo',\n",
    "            lon = [ aerial.iloc[i,9], aerial.iloc[i,16] ],\n",
    "            lat = [ aerial.iloc[i,8], aerial.iloc[i,15] ],\n",
    "            mode = 'lines',\n",
    "            line = dict(\n",
    "                width = 0.7,\n",
    "                color = 'black',\n",
    "            ),\n",
    "            opacity = 0.6,\n",
    "        )\n",
    "    )\n",
    "    \n",
    "layout = dict(\n",
    "    title = 'Bombing Paths from Attacker Country to Target ',\n",
    "    hovermode='closest',\n",
    "    geo = dict(showframe=False, showland=True, showcoastlines=True, showcountries=True,\n",
    "               countrywidth=1, projection=dict(type='Mercator'),\n",
    "              landcolor = 'rgb(217, 217, 217)',\n",
    "              subunitwidth=1,\n",
    "              showlakes = True,\n",
    "              lakecolor = 'rgb(255, 255, 255)',\n",
    "              countrycolor=\"rgb(5, 5, 5)\")\n",
    ")\n",
    "    \n",
    "fig = dict( data=flight_paths + airports+targets, layout=layout )\n",
    "iplot( fig )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90aa69f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Theater of Operations\n",
    "print(aerial['Theater of Operations'].value_counts())\n",
    "plt.figure(figsize=(22,10))\n",
    "sns.countplot(aerial['Theater of Operations'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5f908c",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_station_id = weather_station_location[weather_station_location.NAME == \"BINDUKURI\"].WBAN \n",
    "weather_bin = weather[weather.STA == 32907]\n",
    "weather_bin[\"Date\"] = pd.to_datetime(weather_bin[\"Date\"])\n",
    "plt.figure(figsize=(22,10))\n",
    "plt.plot(weather_bin.Date,weather_bin.MeanTemp)\n",
    "plt.title(\"Mean Temperature of Bindukuri Area\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Mean Temperature\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aee0c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "aerial = pd.read_csv(\"../input/world-war-ii/operations.csv\")\n",
    "aerial[\"year\"] = [ each.split(\"/\")[2] for each in aerial[\"Mission Date\"]]\n",
    "aerial[\"month\"] = [ each.split(\"/\")[0] for each in aerial[\"Mission Date\"]]\n",
    "aerial = aerial[aerial[\"year\"]>=\"1943\"]\n",
    "aerial = aerial[aerial[\"month\"]>=\"8\"]\n",
    "\n",
    "aerial[\"Mission Date\"] = pd.to_datetime(aerial[\"Mission Date\"])\n",
    "\n",
    "attack = \"USA\"\n",
    "target = \"BURMA\"\n",
    "city = \"KATHA\"\n",
    "\n",
    "aerial_war = aerial[aerial.Country == attack]\n",
    "aerial_war = aerial_war[aerial_war[\"Target Country\"] == target]\n",
    "aerial_war = aerial_war[aerial_war[\"Target City\"] == city]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc13043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I get very tired while writing this part, so sorry for this dummy code But I guess you got the idea\n",
    "liste = []\n",
    "aa = []\n",
    "for each in aerial_war[\"Mission Date\"]:\n",
    "    dummy = weather_bin[weather_bin.Date == each]\n",
    "    liste.append(dummy[\"MeanTemp\"].values)\n",
    "aerial_war[\"dene\"] = liste\n",
    "for each in aerial_war.dene.values:\n",
    "    aa.append(each[0])\n",
    "\n",
    "# Create a trace\n",
    "trace = go.Scatter(\n",
    "    x = weather_bin.Date,\n",
    "    mode = \"lines\",\n",
    "    y = weather_bin.MeanTemp,\n",
    "    marker = dict(color = 'rgba(16, 112, 2, 0.8)'),\n",
    "    name = \"Mean Temperature\"\n",
    ")\n",
    "trace1 = go.Scatter(\n",
    "    x = aerial_war[\"Mission Date\"],\n",
    "    mode = \"markers\",\n",
    "    y = aa,\n",
    "    marker = dict(color = 'rgba(16, 0, 200, 1)'),\n",
    "    name = \"Bombing temperature\"\n",
    ")\n",
    "layout = dict(title = 'Mean Temperature --- Bombing Dates and Mean Temperature at this Date')\n",
    "data = [trace,trace1]\n",
    "\n",
    "fig = dict(data = data, layout = layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bf46ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82ab7fd1",
   "metadata": {},
   "source": [
    "\n",
    "# Time Series Prediction with ARIMA\n",
    "We will use most used method ARIMA\n",
    "\n",
    "ARIMA : AutoRegressive Integrated Moving Average. I will explain it detailed at next parts.\n",
    "\n",
    "The way that we will follow:\n",
    "What is Time Series ?\n",
    "\n",
    "Stationarity of a Time Series\n",
    "\n",
    "Make a Time Series Stationary?\n",
    "\n",
    "Forecasting a Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0348d3",
   "metadata": {},
   "source": [
    "\n",
    "# What is time series?\n",
    "Time series is a collection of data points that are collected at constant time intervals.\n",
    "\n",
    "It is time dependent.\n",
    "\n",
    "Most of time series have some form of seasonality trends. For example, if we sale ice cream, most probably there will be higher sales in summer seasons. Therefore, this time series has seasonality trends.\n",
    "\n",
    "Another example, lets think we dice one time every day during 1 year. As you guess, there will be no scenario like that number six is appeared mostly in summer season or number five is mostly appeared in January. Therefore, this time series does not have seasonality trends."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea5c3b5",
   "metadata": {},
   "source": [
    "\n",
    "# Stationarity of a Time Series\n",
    "There are three basic criterion for a time series to understand whether it is stationary series or not.\n",
    "\n",
    "Statistical properties of time series such as mean, variance should remain constant over time to call time series is stationary\n",
    "\n",
    "constant mean\n",
    "\n",
    "constant variance\n",
    "\n",
    "autocovariance that does not depend on time. autocovariance is covariance between time series and lagged time series.\n",
    "\n",
    "Lets visualize and check seasonality trend of our time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bb4da4",
   "metadata": {},
   "outputs": [],
   "source": [
    " Mean temperature of Bindikuri area\n",
    "plt.figure(figsize=(22,10))\n",
    "plt.plot(weather_bin.Date,weather_bin.MeanTemp)\n",
    "plt.title(\"Mean Temperature of Bindukuri Area\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Mean Temperature\")\n",
    "plt.show()\n",
    "\n",
    "# lets create time series from weather \n",
    "timeSeries = weather_bin.loc[:, [\"Date\",\"MeanTemp\"]]\n",
    "timeSeries.index = timeSeries.Date\n",
    "ts = timeSeries.drop(\"Date\",axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31b14be",
   "metadata": {},
   "source": [
    "As you can see from plot above, our time series has seasonal variation. In summer, mean temperature is higher and in winter mean temperature is lower for each year.\n",
    "Now lets check stationary of time series. We can check stationarity using the following methods:\n",
    "\n",
    "Plotting Rolling Statistics: We have a window lets say window size is 6 and then we find rolling mean and variance to check stationary.\n",
    "\n",
    "Dickey-Fuller Test: The test results comprise of a Test Statistic and some Critical Values for difference confidence levels. If the test statistic is less than the critical value, we can say that time series is stationary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a241f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adfuller library \n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "# check_adfuller\n",
    "def check_adfuller(ts):\n",
    "    # Dickey-Fuller test\n",
    "    result = adfuller(ts, autolag='AIC')\n",
    "    print('Test statistic: ' , result[0])\n",
    "    print('p-value: '  ,result[1])\n",
    "    print('Critical Values:' ,result[4])\n",
    "# check_mean_std\n",
    "def check_mean_std(ts):\n",
    "    #Rolling statistics\n",
    "    rolmean = pd.rolling_mean(ts, window=6)\n",
    "    rolstd = pd.rolling_std(ts, window=6)\n",
    "    plt.figure(figsize=(22,10))   \n",
    "    orig = plt.plot(ts, color='red',label='Original')\n",
    "    mean = plt.plot(rolmean, color='black', label='Rolling Mean')\n",
    "    std = plt.plot(rolstd, color='green', label = 'Rolling Std')\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Mean Temperature\")\n",
    "    plt.title('Rolling Mean & Standard Deviation')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "# check stationary: mean, variance(std)and adfuller test\n",
    "check_mean_std(ts)\n",
    "check_adfuller(ts.MeanTemp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde8f2c6",
   "metadata": {},
   "source": [
    "Our first criteria for stationary is constant mean. So we fail because mean is not constant as you can see from plot(black line) above . (no stationary)\n",
    "\n",
    "Second one is constant variance. It looks like constant. (yes stationary)\n",
    "\n",
    "Third one is that If the test statistic is less than the critical value, we can say that time series is stationary. Lets look:\n",
    "\n",
    "test statistic = -1.4 and critical values = {'1%': -3.439229783394421, '5%': -2.86545894814762, '10%': -2.5688568756191392}. Test statistic is bigger than the critical values. (no stationary)\n",
    "\n",
    "As a result, we sure that our time series is not stationary.\n",
    "\n",
    "Lets make time series stationary at the next part."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066e7b10",
   "metadata": {},
   "source": [
    "\n",
    "# Make a Time Series Stationary?\n",
    "As we mentioned before, there are 2 reasons behind non-stationarity of time series\n",
    "\n",
    "Trend: varying mean over time. We need constant mean for stationary of time series.\n",
    "\n",
    "Seasonality: variations at specific time. We need constant variations for stationary of time series.\n",
    "\n",
    "First solve trend(constant mean) problem\n",
    "\n",
    "Most popular method is moving average.\n",
    "\n",
    "Moving average: We have window that take the average over the past 'n' sample. 'n' is window size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7afcc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving average method\n",
    "window_size = 6\n",
    "moving_avg = pd.rolling_mean(ts,window_size)\n",
    "plt.figure(figsize=(22,10))\n",
    "plt.plot(ts, color = \"red\",label = \"Original\")\n",
    "plt.plot(moving_avg, color='black', label = \"moving_avg_mean\")\n",
    "plt.title(\"Mean Temperature of Bindukuri Area\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Mean Temperature\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d371ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_moving_avg_diff = ts - moving_avg\n",
    "ts_moving_avg_diff.dropna(inplace=True) # first 6 is nan value due to window size\n",
    "\n",
    "# check stationary: mean, variance(std)and adfuller test\n",
    "check_mean_std(ts_moving_avg_diff)\n",
    "check_adfuller(ts_moving_avg_diff.MeanTemp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7837c2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# differencing method\n",
    "ts_diff = ts - ts.shift()\n",
    "plt.figure(figsize=(22,10))\n",
    "plt.plot(ts_diff)\n",
    "plt.title(\"Differencing method\") \n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Differencing Mean Temperature\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2dd7f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_diff.dropna(inplace=True) # due to shifting there is nan values\n",
    "# check stationary: mean, variance(std)and adfuller test\n",
    "check_mean_std(ts_diff)\n",
    "check_adfuller(ts_diff.MeanTemp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaff4c02",
   "metadata": {},
   "source": [
    "\n",
    "# Forecasting a Time Series\n",
    "We learn two different methodsthat are moving average and differencing methods to avoid trend and seasonality problem\n",
    "\n",
    "For prediction(forecasting) we will use ts_diff time series that is result of differencing method. There is no reason I only choose it.\n",
    "\n",
    "Also prediction method is ARIMA that is Auto-Regressive Integrated Moving Averages.\n",
    "\n",
    "AR: Auto-Regressive (p): AR terms are just lags of dependent variable. For example lets say p is 3, we will use x(t-1), x(t-2) and x(t-3) to predict x(t)\n",
    "\n",
    "I: Integrated (d): These are the number of nonseasonal differences. For example, in our case we take the first order difference. So we pass that variable and put d=0\n",
    "\n",
    "MA: Moving Averages (q): MA terms are lagged forecast errors in prediction equation.\n",
    "\n",
    "(p,d,q) is parameters of ARIMA model.\n",
    "\n",
    "In order to choose p,d,q parameters we will use two different plots.\n",
    "Autocorrelation Function (ACF): Measurement of the correlation between time series and lagged version of time series.\n",
    "\n",
    "Partial Autocorrelation Function (PACF): This measures the correlation between the time series and lagged version of time series but after eliminating the variations already explained by the intervening comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88bffd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACF and PACF \n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "lag_acf = acf(ts_diff, nlags=20)\n",
    "lag_pacf = pacf(ts_diff, nlags=20, method='ols')\n",
    "# ACF\n",
    "plt.figure(figsize=(22,10))\n",
    "\n",
    "plt.subplot(121) \n",
    "plt.plot(lag_acf)\n",
    "plt.axhline(y=0,linestyle='--',color='gray')\n",
    "plt.axhline(y=-1.96/np.sqrt(len(ts_diff)),linestyle='--',color='gray')\n",
    "plt.axhline(y=1.96/np.sqrt(len(ts_diff)),linestyle='--',color='gray')\n",
    "plt.title('Autocorrelation Function')\n",
    "\n",
    "# PACF\n",
    "plt.subplot(122)\n",
    "plt.plot(lag_pacf)\n",
    "plt.axhline(y=0,linestyle='--',color='gray')\n",
    "plt.axhline(y=-1.96/np.sqrt(len(ts_diff)),linestyle='--',color='gray')\n",
    "plt.axhline(y=1.96/np.sqrt(len(ts_diff)),linestyle='--',color='gray')\n",
    "plt.title('Partial Autocorrelation Function')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06152dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARIMA LİBRARY\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from pandas import datetime\n",
    "\n",
    "# fit model\n",
    "model = ARIMA(ts, order=(1,0,1)) # (ARMA) = (1,0,1)\n",
    "model_fit = model.fit(disp=0)\n",
    "\n",
    "# predict\n",
    "start_index = datetime(1944, 6, 25)\n",
    "end_index = datetime(1945, 5, 31)\n",
    "forecast = model_fit.predict(start=start_index, end=end_index)\n",
    "\n",
    "# visualization\n",
    "plt.figure(figsize=(22,10))\n",
    "plt.plot(weather_bin.Date,weather_bin.MeanTemp,label = \"original\")\n",
    "plt.plot(forecast,label = \"predicted\")\n",
    "plt.title(\"Time Series Forecast\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Mean Temperature\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c4c166",
   "metadata": {},
   "source": [
    "lets predict and visualize all path and find mean squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248d1332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict all path\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# fit model\n",
    "model2 = ARIMA(ts, order=(1,0,1)) # (ARMA) = (1,0,1)\n",
    "model_fit2 = model2.fit(disp=0)\n",
    "forecast2 = model_fit2.predict()\n",
    "error = mean_squared_error(ts, forecast2)\n",
    "print(\"error: \" ,error)\n",
    "# visualization\n",
    "plt.figure(figsize=(22,10))\n",
    "plt.plot(weather_bin.Date,weather_bin.MeanTemp,label = \"original\")\n",
    "plt.plot(forecast2,label = \"predicted\")\n",
    "plt.title(\"Time Series Forecast\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Mean Temperature\")\n",
    "plt.legend()\n",
    "plt.savefig('graph.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0d37e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar = pd.DataFrame(index = pd.date_range('2013-01-01', '2017-08-31')).to_period('D')\n",
    "oil = pd.read_csv('../input/store-sales-time-series-forecasting/oil.csv',\n",
    "                  parse_dates = ['date'], infer_datetime_format = True,\n",
    "                  index_col = 'date').to_period('D')\n",
    "oil['avg_oil'] = oil['dcoilwtico'].rolling(7).mean()\n",
    "calendar = calendar.join(oil.avg_oil)\n",
    "calendar['avg_oil'].fillna(method = 'ffill', inplace = True)\n",
    "calendar.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14315c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_pacf(calendar.avg_oil, lags = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03d6fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = train.groupby(\"family\").sales.mean().sort_values(ascending = False).reset_index()\n",
    "px.bar(a, y = \"family\", x=\"sales\", color = \"family\", title = \"Top selling product families\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99293135",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdate = '2017-04-30'\n",
    "x=x.loc[sdate:]\n",
    "y=y.loc[sdate:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b65d20",
   "metadata": {},
   "source": [
    " Train final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7e19e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.compose import TransformedTargetRegressor, ColumnTransformer\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "lnr_reg = TransformedTargetRegressor(\n",
    "    regressor = LinearRegression(fit_intercept = True, n_jobs = -1),\n",
    "    func=np.log1p,\n",
    "    inverse_func=np.expm1\n",
    ")\n",
    "lnr = make_pipeline(\n",
    "    ColumnTransformer([(\"drop_f\", \"drop\", ext_features)], remainder=\"passthrough\"),\n",
    "    PowerTransformer(),\n",
    "    lnr_reg\n",
    ")\n",
    "\n",
    "lnr.fit(x, y)\n",
    "yfit_lnr = pd.DataFrame(lnr.predict(x), index = x.index, columns = y.columns).clip(0.)\n",
    "ypred_lnr = pd.DataFrame(lnr.predict(xtest), index = xtest.index, columns = y.columns).clip(0.)\n",
    "\n",
    "y_ = y.stack(['store_nbr', 'family'])\n",
    "y_['lnr'] = yfit_lnr.stack(['store_nbr', 'family'])['sales']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d31108",
   "metadata": {},
   "source": [
    "# 4.2 Final model - Custom Regressor\n",
    "What CustomRegressor is doing here - it's fiting individual model on EVERY component of Multivariate TS - single TS as combination FAMILY x STORE. And there will be 1658 independent models (excluding Zero forecast).\n",
    "In our analysis on cross product correlations, we decided to introduce 3 groups of models in CustomRegressor, customized for specific product family set:\n",
    "\n",
    "Selected non stationary time-series with low correlations: GradientBoostingRegressor + ExtraTreesRegressor voting ensemble model\n",
    "High sales product categories (TOP 40% by sales): K-nearest neighbors regressor + Bayesian regressor + Ridge + SVR voting ensemble model\n",
    "Product families enriched with new external features: simpe Ridge regressor\n",
    "Low sales product categories (BOTTOM 60% by sales): Ridge regressor + SVR voting ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3287a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.linear_model import Ridge, ARDRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import ExtraTreesRegressor, GradientBoostingRegressor\n",
    "from sklearn.ensemble import BaggingRegressor, VotingRegressor\n",
    "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# SEED for reproducible result\n",
    "SEED = 5\n",
    "\n",
    "class CustomRegressor():\n",
    "    \n",
    "    def __init__(self, ext_features = None, knn_features = None, non_st_ts = None, low_sales_ts = None, enriched_ts_map = None, n_jobs=-1):\n",
    "        \n",
    "        self.n_jobs = n_jobs\n",
    "        self.ext_features = ext_features\n",
    "        self.knn_features = knn_features\n",
    "        self.non_st_ts = non_st_ts\n",
    "        self.low_sales_ts = low_sales_ts\n",
    "        self.enriched_ts_map = enriched_ts_map\n",
    "        self.estimators_ = None\n",
    "        self.product_names_ = None\n",
    "        self.store_names_ = None\n",
    "        \n",
    "    def _estimator_(self, X, y):\n",
    "        warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "        \n",
    "        # We remove external features for the products, which univariate TS were note used during feature search & enrichment\n",
    "        # As these features won't be relevant for the rest of product families and might decrease accuracy\n",
    "        remove_ext_features = ColumnTransformer([(\"drop_f\", \"drop\", self.ext_features)],remainder=\"passthrough\")\n",
    "        \n",
    "        if y.name[2] in self.non_st_ts:\n",
    "            b1 = GradientBoostingRegressor(n_estimators = 175, max_depth=3, loss='huber', random_state=SEED)\n",
    "            r1 = ExtraTreesRegressor(n_estimators = 250, n_jobs=self.n_jobs, random_state=SEED)\n",
    "            b2 = BaggingRegressor(base_estimator=r1,\n",
    "                                  n_estimators=10,\n",
    "                                  n_jobs=self.n_jobs,\n",
    "                                  random_state=SEED)\n",
    "            model = make_pipeline(\n",
    "                remove_ext_features,\n",
    "                VotingRegressor ([('gbr', b1), ('et', b2)])\n",
    "            )      \n",
    "        elif y.name[2] in self.enriched_ts_map.keys():\n",
    "            ## use only external features selected for this particular TS\n",
    "            ext_features_ = [col for col in self.ext_features if col not in self.enriched_ts_map[y.name[2]]]\n",
    "            remove_ext_features_ = ColumnTransformer([(\"drop_f\", \"drop\", ext_features_)],remainder=\"passthrough\")\n",
    "            \n",
    "            # Yeo-Johnson transform for X to reshape X distribution closer to normal (Gaussian) for linear model\n",
    "            power_tr = PowerTransformer()\n",
    "            \n",
    "            # Log(y) + 1 transform to reshape y distribution closer to normal for linear model,\n",
    "            # then round transform - as  LIQUOR,WINE,BEER and CLEANING sales must be integer\n",
    "            ridge = TransformedTargetRegressor(\n",
    "                regressor = Ridge(fit_intercept=True, solver='auto', alpha=0.7, normalize=True, random_state=SEED),\n",
    "                func=np.log1p,\n",
    "                inverse_func=np.expm1\n",
    "            )\n",
    "            ridge_round_to_int = TransformedTargetRegressor(\n",
    "                regressor=ridge,\n",
    "                inverse_func=np.rint\n",
    "            )\n",
    "            model = make_pipeline(\n",
    "                remove_ext_features_,\n",
    "                power_tr,\n",
    "                ridge_round_to_int\n",
    "            )\n",
    "            \n",
    "        elif y.name[2] in self.low_sales_ts:\n",
    "            ridge = TransformedTargetRegressor(\n",
    "                regressor = Ridge(fit_intercept=True, solver='auto', alpha=0.75, normalize=True, random_state=SEED),\n",
    "                func=np.log1p,\n",
    "                inverse_func=np.expm1\n",
    "            )\n",
    "            svr = TransformedTargetRegressor(\n",
    "                regressor = SVR(C = 0.2, kernel = 'rbf'),\n",
    "                func=np.log1p,\n",
    "                inverse_func=np.expm1\n",
    "            )\n",
    "            model = VotingRegressor([('ridge', ridge), ('svr', svr)])\n",
    "        else:\n",
    "            ridge = make_pipeline(\n",
    "                remove_ext_features,\n",
    "                TransformedTargetRegressor(\n",
    "                    regressor = Ridge(fit_intercept=True, solver='auto', alpha=0.6, normalize=True, random_state=SEED),\n",
    "                    func=np.log1p,\n",
    "                    inverse_func=np.expm1)\n",
    "            )\n",
    "            svr = make_pipeline(\n",
    "                remove_ext_features,\n",
    "                TransformedTargetRegressor(\n",
    "                    regressor = SVR(C = 0.2, kernel = 'rbf'),\n",
    "                    func=np.log1p,\n",
    "                    inverse_func=np.expm1)\n",
    "            )\n",
    "            # We'll use specific feature set for KNN to cluster observations in a way to catch week and intra-week seasonality\n",
    "            knn = make_pipeline(\n",
    "                ColumnTransformer([(\"selector\", \"passthrough\", self.knn_features)], remainder=\"drop\"),\n",
    "                PowerTransformer(),\n",
    "                KNeighborsRegressor(n_neighbors=3, n_jobs=self.n_jobs)\n",
    "            )\n",
    "            ard = make_pipeline(\n",
    "                remove_ext_features,\n",
    "                TransformedTargetRegressor(\n",
    "                    regressor = ARDRegression(fit_intercept=True, normalize=True, n_iter=300),\n",
    "                    func=np.log1p,\n",
    "                    inverse_func=np.expm1)\n",
    "            )\n",
    "            estimators = [\n",
    "                ('ridge', ridge),\n",
    "                ('svr', svr),\n",
    "                (\"ard\", ard),\n",
    "                (\"knn\",knn)\n",
    "            ]\n",
    "            model = VotingRegressor(estimators)\n",
    "            \n",
    "        model.fit(X, y)\n",
    "        return model\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        print(\"Fit stage...\")\n",
    "        self.product_names_ = [str(y.iloc[:, i].name[2]) for i in range(y.shape[1])]\n",
    "        self.store_names_ = [str(y.iloc[:, i].name[1]) for i in range(y.shape[1])]\n",
    "        self.estimators_ = []\n",
    "        for i, n in tqdm(enumerate(self.product_names_)):\n",
    "            estimator_ = self._estimator_(\n",
    "                # select as features only predictions of product sales in the same store or same product in other stores\n",
    "                X.filter(\n",
    "                    regex= n + \"'\\)$|\\(\\d|^[a-zA-Z_0-9., ]+$|\\('sales', '\" + str(y.iloc[:, i].name[1]) + \"',\",\n",
    "                    axis=1,\n",
    "                ),\n",
    "                y.iloc[:, i],\n",
    "            )\n",
    "            self.estimators_.append(estimator_)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        print(\"Prediction stage...\")\n",
    "        y_pred = []\n",
    "        for e, n, m in tqdm(zip(self.estimators_, self.product_names_, self.store_names_)):\n",
    "            y_pred_ = e.predict(\n",
    "                # select as features only predictions of product sales in the same store or same product in other stores\n",
    "                X.filter(\n",
    "                    regex= n + \"'\\)$|\\(\\d|^[a-zA-Z_0-9., ]+$|\\('sales', '\" + m + \"',\",\n",
    "                    axis=1,\n",
    "                )\n",
    "            )\n",
    "            y_pred.append(y_pred_)\n",
    "            \n",
    "        return np.stack(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5eea780",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "# manual selection for KNN regression\n",
    "knn_features = list(int_features - set(['oil_lags2', 'oil_lags1',\"trend\"]))\n",
    "model = CustomRegressor(ext_features, knn_features, non_st_ts, low_sales_ts, enriched_ts_map, n_jobs=-1)\n",
    "model.fit(x, y)\n",
    "\n",
    "y_pred = pd.DataFrame(model.predict(x), index=x.index, columns=y.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c751cf2b",
   "metadata": {},
   "source": [
    "# Submit prediction with enriched features and calculate final leaderbord progress\n",
    "Let's quickly estimate model accuracy (no cross validation!) and submit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c2cebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import mean_squared_log_error as msle\n",
    "y_pred = y_pred.stack(['store_nbr', 'family']).clip(0.)\n",
    "y_ = y.stack(['store_nbr', 'family']).clip(0.)\n",
    "y_['pred'] = y_pred.values\n",
    "print(y_.groupby('family').apply(lambda r : np.sqrt(np.sqrt(msle(r['sales'], r['pred'])))))\n",
    "print('RMSLE : ', np.sqrt(np.sqrt(msle(y_['sales'], y_['pred']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941fb92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "ypred = pd.DataFrame(model.predict(xtest), index = xtest.index, columns = y.columns).clip(0.)\n",
    "ypred = ypred.stack(['store_nbr', 'family'])\n",
    "ypred = ypred.append(zero_prediction).sort_index()\n",
    "sub = pd.read_csv('../input/store-sales-time-series-forecasting/sample_submission.csv')\n",
    "sub['sales'] = ypred.values\n",
    "sub.to_csv('submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdaa398",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9286785",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
